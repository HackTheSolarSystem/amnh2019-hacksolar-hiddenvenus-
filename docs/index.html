<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>README</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
      html {
          margin: auto;
          max-width: 50rem;
      }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="lotta-research">Lotta Research</h2>
<h3 id="adressing-the-hidden-face-of-venus">Adressing <a href="https://github.com/amnh/HackTheSolarSystem/wiki/The-Hidden-Face-of-Venus">The Hidden Face of Venus</a></h3>
<h3 id="created-by-team-team">Created By Team Team</h3>
<ul>
<li>Jennifer Shin
<ul>
<li><a href="https://www.linkedin.com/in/jennshin" class="uri">https://www.linkedin.com/in/jennshin</a></li>
<li><a href="https://github.com/jennshin" class="uri">https://github.com/jennshin</a></li>
</ul></li>
<li>Adam Ibrahim
<ul>
<li><a href="https://www.linkedin.com/in/adam-ibrahim/" class="uri">https://www.linkedin.com/in/adam-ibrahim/</a></li>
<li><a href="https://github.com/beelzebielsk" class="uri">https://github.com/beelzebielsk</a></li>
</ul></li>
</ul>
<p>Status Report:</p>
<p>We can read in the binary data from a single orbit from the Magellan Mission and view it as greyscale images.</p>
<p>We’re currently researching leads on what to do next. There are existing tools for generating DEMs, but would not be directly compatible with the Magellan data. There’s also existing work on the Magellan data we could improve on. Finally, there are tools to create DEMs used for later planetary missions; perhaps we could make use of those by making them compatible with magellan data.</p>
<h3 id="solution-description">Solution Description</h3>
<p>The current solution is <em>research</em>. Reading. Lots of it.</p>
<p>The file <code>images.py</code> is a demo of the F-BIDR file reading work, and can extract a grayscale image from Magellan Mission data, if some is placed in the repostiory root directory.</p>
<h1 id="progress">Progress</h1>
<p>This will describe our current understanding of the project. What we know so far. Future work is covering what we know we don’t know, and looking for tools that could help us.</p>
<h2 id="key-sources">Key Sources</h2>
<ul>
<li>Data from the Magellan mission is available <a href="http://pds-geosciences.wustl.edu/mgn/mgn-v-rdrs-5-bidr-full-res-v1/">here</a></li>
<li>The Magellan Software Interface Specification (SIS) for the F-BIDR files located in <code>papers+documents/MGN-FBIDR-SIS-SDPS-101-RevE.pdf</code>. Each F-BIDR is a collection of 20 files (and some extra metadata files, whose extension is <code>.lbl</code>) which contain both image data and metadata about the image and spacecraft. This document describes the binary format of some of the files in detail (Files 12-19), and also describes the meaning of the information, but is not sufficient alone for interpretation. From here on in, will be called just <em>FBIDR SIS</em>.</li>
<li>The Guide to Magellan Image Interpretation located in <code>papers+documents/19940013181.pdf</code>. This is broader in scope and less terse than the F-BIDR SIS. It goes over details of the mission, contains a lot of definitions for important terms and concepts in the way that satellites function, and the way that the radar on the satellite functions as well. For actual image interpretation, Chapters 2 and 5 of this document are valuable. From here on, will be called just <em>Magellan Guide</em>.</li>
</ul>
<h2 id="problems-what-are-we-trying-to-overcome">Problems: what are we trying to overcome?</h2>
<p>The Magellan data was collected between about 1991 and 1993. The mission happened before later tools for stereogrammetry (getting elevation data from a pair of images from different perspectives of the same region) were developed. So the data isn’t immediately usable with known tools for performing stereogrammetry. Common stereogrammetry tools aren’t compatible with it for futher reasons: this is radar data, not optical data and must be handled a little differently; and most tools can’t handle planetary data. TODO: Explain this a little better; is it the scale of planetary data that’s the problem? What’s the difference briefly b/w photogrammetry and radargrammetry? Real brief.</p>
<p>Magellan carried an altimeter on board, but the resolution of the height data is very large (on the order of 10km in between each sample), and is not always reliable. In mountainous regions, errors in altitude can be as large as a kilometer.<br />
TODO: Source.</p>
<p>Lastly, the actual images are not optical images; they are not created by reflected visible light from a surface. This was done for different reasons, but one of them being thick clouds that cover Venus. Radiation that could penetrate the clouds had to be used.</p>
<p>So for an accurate map of elevation for Venus, techniques that rely on images of the surface have to be used, because that’s what’s available, could produce results more accurate and at higher resolutions than the on-board altimeter.</p>
<h2 id="what-do-the-pictures-mean-and-where-are-they-in-the-data">What do the pictures mean, and where are they in the data?</h2>
<p>There’s supposed to be image data of Venus’ surface. Where?</p>
<p>Each F-BIDR is a collection of 20 files, much of which is metadata. Files 12-19, as specified in the FBIDR SIS are comprised of pieces called <em>logical records</em>. Files 13 and 15 have logical records which contain both satellite metadata and image data from the radar. The satellite metadata helps us understand which part of Venus’ surface the image corresponds to, so that the pieces can be joined together into larger pictures which represent a whole orbit, or multiple orbits.</p>
<p>Each pixel represents <span class="math inline">\(75m^2\)</span> of Venus’ surface. They’re 75m wide and tall. A radar on Magellan emitted radiation from a small dish and captured some of the radiation back a small amount of time later. The intensity of the pixel is the intensity of the reflected radiation. What precisely this means we can’t quite say yet. More reading to do. It’s easier to start off with what it <em>isn’t</em>.</p>
<ul>
<li>It’s not brightness of the surface. The radiation isn’t visible and the radar wasn’t affected by visible light (we assume).</li>
<li>It has nothing to do with color of the surface (we assume).</li>
</ul>
<p>What the intensities tell us is how capable a given patch of land was at reflecting the radiation back toward the satellite. Chapter 5 of the Magellan Guide goes into more detail about this, but here’s a summary. First, an image describing various terms on satellite orientation toward the surface. The <em>SAR</em> is the radar dish on the magellan satellite. If I talk about satellite orientation, I’m really talking about the orientation of this dish.</p>
<figure>
<img src="readme-images/satellite-info.png" alt="Demonstration of satellite orientation terms" /><figcaption>Demonstration of satellite orientation terms</figcaption>
</figure>
<p>A note: incidence angle and look angle are <em>not</em> the same. On the radar swath is a line that crosses it, along with a squiggly. The line represents an assumed reference surface, and the squiggly is the actual surface. The reference surface is flat in the picture, but it doesn’t have to be. And actual surfaces often aren’t flat. The picture-in-picture insert describes this well: there’s the <em>local</em> incidence angle, which is the actual incidence angle the radiation makes with the actual (squiggly) ground.</p>
<p>There’s a few things that affect how capable a patch of land is at reflecting radiation: the <em>incidence angle</em> (the angle between the radiation and the surface struck by it), the roughness of the ground, and the patch having very reflective materials. If a patch has more reflective materials, then generally the intensity of the pixel representing that patch will be greater. For the other two parameters, the explanation is less straightforward; incidence angle and roughness have an interplay, as shown in the following 2 figures:</p>
<figure>
<img src="readme-images/incidence-angle-land-demo.png" alt="Demonstration of reflectivity of 2 different surfaces" /><figcaption>Demonstration of reflectivity of 2 different surfaces</figcaption>
</figure>
<p>In the very top image, the surface is flat so it acts like a mirror. Radiation hits it and then bounces away. If the incidence angle is small, then the radiation will probably bounce back toward the radar causing very bright intensity (radar-bright); if the incidence angle is large, then the flat ground will show up dark (radar-dark).</p>
<p>The more rough the ground, the less intensity will shoot right back at the radar in any one direction because the radiation will be spread out in different directions; however regardless of the incidence angle, radiation is more likely to go back to the radar than miss it. The following graph shows this in more detail. Higher radar backscatter means higher pixel intensity.</p>
<figure>
<img src="readme-images/incidence-angle-reflection-graph.png" alt="Backscatter vs incidence angle for different surfaces" /><figcaption>Backscatter vs incidence angle for different surfaces</figcaption>
</figure>
<p>Notice that the graph for rough ground changes less with incidence angle while the graph for flat ground changes a lot with incidence angle.</p>
<p>Here’s two radar images of some mountains in California which display the difference between different incidence angles. They’re of the same region; the dark bottom of the top image is not the night sky (we thought it was for a moment), but <em>flat ground</em> from a high incidence angle (<span class="math inline">\(50\deg\)</span>). There are bright dots in the dark region of the top photo; look for them in the bottom photo to be able to compare the two regions. The pictures are basically the same scale and width and height.</p>
<figure>
<img src="readme-images/incidence-angle-land-effects.png" alt="Comparison of incidence angle effects at California Mountains" /><figcaption>Comparison of incidence angle effects at California Mountains</figcaption>
</figure>
<p>Left to Explain:</p>
<ul>
<li>Maybe the behavior of the satllite? How it spun around the planet, how it recorded radar data. A diagram of the orbit inclincation angle could help, and the rotation direction of Venus. I drew one of those already. This could extend into a discussion on doppler/range because that may be important to the work anyway.</li>
<li>Cycles, what was gained from each cycle.</li>
<li>How we piece together pieces of an image.</li>
<li>Binary format of the data? Eh…</li>
<li>The format of the files, at least. And their role. I could at least point them to a good page in the FBIDR SIS.</li>
<li>Explanation of image metadata, important pieces. Explanation of the different coordinate systems (sinusoidal pixel/line and spherical lat/lon).</li>
<li>What sources of image data are available and what do they provide? Why work with the original? This is a bit of a pitfall for a newbie, because there’s lots of stuff that looks like it should be fine and makes the project a cinch, but there’s some sort of trouble with it. Inaccurate, or produced poorly or something like that.
<ul>
<li>Altimeter inaccuracy</li>
<li>Improved ephemeris data, where to get it how to use it</li>
</ul></li>
<li>Maybe include a list of sources, like a report.</li>
<li>The thursts, man! The <em>thrusts</em>! What are we currently researching in a little more detail.</li>
</ul>
</body>
</html>
